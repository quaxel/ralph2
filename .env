# Ralph Pipeline Configuration
PORT=3000

# AI Provider Selection: 'openai', 'lmstudio', 'ollama'
CODEX_PROVIDER=lmstudio

# Model Selection
# For OpenAI: 'gpt-4o', 'o3-mini'
# For LM Studio: 'zai-org/glm-4.7-flash' (or your loaded model name)
CODEX_MODEL=zai-org/glm-4.7-flash

# API Credentials & Endpoints
# OpenAI Key (Required if provider is 'openai')
OPENAI_API_KEY=sk-xxxx...

# LM Studio Base URL (Default: http://localhost:1234/v1/chat/completions)
LMSTUDIO_API_BASE=http://localhost:1234/v1/chat/completions

# Ollama Base URL (Default: http://localhost:11434/v1/chat/completions)
OLLAMA_API_BASE=http://localhost:11434/v1/chat/completions
